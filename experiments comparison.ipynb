{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e11331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tokenize_uk import tokenize_words\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import numpy as np\n",
    "from tree_stem import stem_word\n",
    "\n",
    "stop = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b273b13",
   "metadata": {},
   "source": [
    "## Contents:\n",
    "* [Fragment-based scores](#first-bullet)\n",
    "* [BERTScore](#second-bullet)\n",
    "* [ROUGE scores](#third-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837b14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd203a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fragments():\n",
    "\n",
    "    subsequences = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len_summary:\n",
    "        res = (0,0)\n",
    "\n",
    "        j = 0\n",
    "        while j < len_article:\n",
    "\n",
    "            if seq_1[i] == seq_2[j]:\n",
    "                i_i = i\n",
    "                j_j = j\n",
    "                while i_i < len_summary and j_j < len_article and seq_1[i_i] == seq_2[j_j]:\n",
    "                    j_j += 1\n",
    "                    i_i += 1\n",
    "                old_length = res[1] - res[0]\n",
    "                if i_i - i > old_length:\n",
    "                    res = (i, i_i)\n",
    "                j = j_j\n",
    "            else:\n",
    "                j += 1\n",
    "\n",
    "        cur_length = res[1] - res[0]\n",
    "        i += max([cur_length, 1])\n",
    "        if cur_length:\n",
    "            subsequences.append(res)\n",
    "\n",
    "    return(subsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc68d2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coverage():\n",
    "    \n",
    "    extractive_fragment_length = 0\n",
    "\n",
    "    for s in fragments:\n",
    "        extractive_fragment_length += s[1] - s[0]    \n",
    "    \n",
    "    return extractive_fragment_length / len(summary_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817bb81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_density():\n",
    "\n",
    "    extractive_fragment_length = 0\n",
    "\n",
    "    for s in fragments:\n",
    "        l = s[1] - s[0]\n",
    "        extractive_fragment_length += l**2\n",
    "\n",
    "    return extractive_fragment_length / len(summary_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f7e397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f6ffbf",
   "metadata": {},
   "source": [
    "## Fragments-based scores <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e9c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d360fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = [\"experiment0\",\"experiment1\",\"experiment2\",\"experiment2_2\",\"experiment3\",\"experiment4\", \"experiment5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in versions:\n",
    "    DF_NAME = f\"results/{version}.csv\"\n",
    "    VERSION = version\n",
    "\n",
    "    df = pd.read_csv(DF_NAME, index_col=0)\n",
    "    \n",
    "    \n",
    "    for ind, row in df.iterrows():\n",
    "\n",
    "        summary = row[\"output\"]\n",
    "        article = row[\"2\"]\n",
    "\n",
    "\n",
    "        seq_1 = tokenize_words(summary)\n",
    "        seq_1 = [i for i in seq_1 if i not in stop]\n",
    "        summary_tokens = seq_1\n",
    "\n",
    "        seq_2 = tokenize_words(article)\n",
    "        seq_2 = [i for i in seq_2 if i not in stop]\n",
    "        article_tokens = seq_2\n",
    "\n",
    "        len_summary = len(seq_1)\n",
    "        len_article = len(seq_2)\n",
    "\n",
    "        fragments = calculate_fragments()\n",
    "        compression = len_article / len_summary\n",
    "\n",
    "        coverage = calculate_coverage()\n",
    "        abstractivity = 1 - coverage\n",
    "\n",
    "        density = calculate_density()\n",
    "\n",
    "        metrics.append((row[\"1\"], round(compression,4), round(coverage,4), \n",
    "                        round(abstractivity,4), round(density,4), len_summary, len_article, VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame(metrics, columns=[\"post_id\",\"compression\", \"coverage\",\"abstractivity\",\"density\",\"tokens_summary\",\"tokens_article\", \"version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637dad26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d3f77d4",
   "metadata": {},
   "source": [
    "## BERTScore <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(\"test_data.csv\", index_col=0)\n",
    "dd = dd.set_index(\"post_id\")\n",
    "d = dd[\"summary\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5dc6d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7cbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"youscan/ukr-roberta-base\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"youscan/ukr-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d45421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_score(txt, summary):\n",
    "    text1 = txt.lower()\n",
    "    text2 = summary.lower()\n",
    "    inputs1 = tokenizer(text1, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    outputs1 = model(**inputs1, output_hidden_states=True)\n",
    "    outputs2 = model(**inputs2, output_hidden_states=True)\n",
    "\n",
    "    embeddings1 = outputs1.hidden_states[-1].mean(dim=1).detach().numpy()\n",
    "    embeddings2 = outputs2.hidden_states[-1].mean(dim=1).detach().numpy()\n",
    "\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2))\n",
    "    \n",
    "    return(round(similarity[0][0],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eda6bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in versions:\n",
    "    DF_NAME = f\"results/{version}.csv\"\n",
    "    VERSION = version\n",
    "    for ind, row in df.iterrows():\n",
    "\n",
    "        txt = row[\"output\"]\n",
    "        post_id = row[\"1\"]\n",
    "        summary = d[post_id]\n",
    "        try:\n",
    "            bert = calculate_score(txt, summary)\n",
    "        except:\n",
    "            bert = np.nan\n",
    "        metrics_df.loc[(metrics_df[\"version\"]==VERSION)&(metrics_df[\"post_id\"]==post_id),\"bertscore\"] = bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fe080a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68919f38",
   "metadata": {},
   "source": [
    "## ROUGE scores <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d50cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_1(s1, s2):\n",
    "    matches = 0\n",
    "    for w in s1:\n",
    "        if w in s2:\n",
    "            matches +=1\n",
    "    tokens1 = len(s1)\n",
    "    tokens2 = len(s2)\n",
    "\n",
    "    if tokens1 == 0 or tokens2 == 0:\n",
    "        return (0,0,0)\n",
    "    \n",
    "    recall = matches/tokens1\n",
    "    precision = matches/tokens2\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return ((precision, recall, 0))\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision+ recall)\n",
    "    \n",
    "    return ((precision, recall, f1))\n",
    "\n",
    "\n",
    "def rouge_l(s1, s2):    \n",
    "    len_s1 = len(s1)\n",
    "    len_s2 = len(s2)\n",
    "    \n",
    "    if len_s1 == 0 or len_s2 == 0:\n",
    "        return (0,0,0)\n",
    "    \n",
    "    lcs = lcs_length(s1, s2)\n",
    "    \n",
    "    recall = lcs/len_s1\n",
    "    precision = lcs/len_s2\n",
    "\n",
    "    if precision + recall == 0:\n",
    "        return ((precision, recall, 0))\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return ((precision, recall, f1))\n",
    "\n",
    "\n",
    "def lcs_length(a, b):\n",
    "    table = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "    for i, ca in enumerate(a, 1):\n",
    "        for j, cb in enumerate(b, 1):\n",
    "            table[i][j] = (\n",
    "                table[i - 1][j - 1] + 1 if ca == cb else\n",
    "                max(table[i][j - 1], table[i - 1][j]))\n",
    "    return table[-1][-1]\n",
    "\n",
    "def rouge_2(s1, s2):\n",
    "    \n",
    "    bigrams1 = list(zip(s1, s1[1:]))\n",
    "    bigrams2 = list(zip(s2, s2[1:]))    \n",
    "    matches = 0\n",
    "\n",
    "    for w in bigrams1:\n",
    "        if w in bigrams2:\n",
    "            matches +=1\n",
    "    tokens1 = len(bigrams1)\n",
    "    tokens2 = len(bigrams2)\n",
    "    \n",
    "    if tokens1 == 0 or tokens2 == 0:\n",
    "        return (0,0,0)\n",
    "    recall = matches/tokens1\n",
    "\n",
    "    precision = matches/tokens2\n",
    "    if precision + recall == 0:\n",
    "        return ((precision, recall, 0))\n",
    "\n",
    "    f1 = 2 * (precision * recall) / (precision+ recall)    \n",
    "    return ((precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_scores(reference, candidate):\n",
    "\n",
    "    s1 = tokenize_words(reference)\n",
    "    s2 = tokenize_words(candidate)\n",
    "\n",
    "    s1 = [stem_word(i) for i in s1 if i not in stop]\n",
    "    s2 = [stem_word(i) for i in s2 if i not in stop]\n",
    "\n",
    "    r1 = rouge_1(s1,s2)\n",
    "    r2 = rouge_2(s1,s2)\n",
    "\n",
    "    r_l = rouge_l(s1,s2)\n",
    "\n",
    "    return(r1, r2, r_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36ead4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for version in versions:\n",
    "    DF_NAME = f\"results/{version}.csv\"\n",
    "    VERSION = version\n",
    "    for ind, row in df.iterrows():\n",
    "\n",
    "        txt = row[\"output\"]\n",
    "        post_id = row[\"1\"]\n",
    "        summary = d[post_id]\n",
    "        try:\n",
    "            r = calculate_scores(summary,txt)\n",
    "\n",
    "            r1 = r[0][2]\n",
    "            r2 = r[1][2]\n",
    "            rl = r[2][2]\n",
    "        except:\n",
    "            r1,r2,rl = np.nan\n",
    "        metrics_df.loc[(metrics_df[\"version\"]==VERSION)&(metrics_df[\"post_id\"]==post_id),\"r1\"] = r1\n",
    "        metrics_df.loc[(metrics_df[\"version\"]==VERSION)&(metrics_df[\"post_id\"]==post_id),\"r2\"] = r2\n",
    "        metrics_df.loc[(metrics_df[\"version\"]==VERSION)&(metrics_df[\"post_id\"]==post_id),\"rl\"] = rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606f306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.to_csv(\"experiments_comparison.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
